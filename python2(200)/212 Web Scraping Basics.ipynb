{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbf3be3",
   "metadata": {},
   "source": [
    "# Web Scraping Basics\n",
    "**웹 스크래핑 기초**\n",
    "\n",
    "**Duration (수업 시간)**: 3 hours (3시간)  \n",
    "**Structure (구성)**: Lecture & Lab 2 hours + Quiz 1 hour (강의 및 실습 2시간 + 퀴즈 1시간)  \n",
    "**Level (수준)**: Intermediate (중급)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives (학습 목표)\n",
    "\n",
    "By the end of this lesson, students will be able to:\n",
    "이 수업을 마친 후 학생들은 다음을 할 수 있습니다:\n",
    "\n",
    "- Understand what web scraping is and when to use it (웹 스크래핑이 무엇이고 언제 사용하는지 이해)\n",
    "- Use requests library to get web page content (requests 라이브러리를 사용하여 웹 페이지 내용 가져오기)\n",
    "- Parse HTML using BeautifulSoup (BeautifulSoup을 사용하여 HTML 파싱)\n",
    "- Extract specific information from web pages (웹 페이지에서 특정 정보 추출)\n",
    "- Understand web scraping ethics and legal considerations (웹 스크래핑 윤리와 법적 고려사항 이해)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is Web Scraping? (웹 스크래핑이란?)\n",
    "\n",
    "Web scraping is like copying information from websites automatically. Instead of manually copying and pasting, you write code to do it for you.\n",
    "웹 스크래핑은 웹사이트에서 정보를 자동으로 복사하는 것과 같습니다. 수동으로 복사하고 붙여넣기 하는 대신, 코드를 작성해서 대신 해주는 것입니다.\n",
    "\n",
    "### Why Use Web Scraping? (왜 웹 스크래핑을 사용하나요?)\n",
    "\n",
    "- **Save time**: Get lots of data quickly (시간 절약: 많은 데이터를 빠르게 얻기)\n",
    "- **Get current data**: Always get the latest information (최신 데이터 얻기: 항상 최신 정보 얻기)\n",
    "- **Analyze trends**: Compare prices, monitor changes (트렌드 분석: 가격 비교, 변화 모니터링)\n",
    "\n",
    "### Simple Example (간단한 예시)\n",
    "\n",
    "Think of web scraping like having a robot assistant that:\n",
    "웹 스크래핑을 다음과 같은 로봇 어시스턴트가 있는 것처럼 생각해보세요:\n",
    "- Visits websites for you (당신을 위해 웹사이트 방문)\n",
    "- Reads the content (내용 읽기)\n",
    "- Copies important information (중요한 정보 복사)\n",
    "- Saves it in a file (파일에 저장)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction to Requests Library (Requests 라이브러리 소개)\n",
    "\n",
    "The **requests** library helps you get web pages from the internet. It's like a web browser, but for your Python code.\n",
    "**requests** 라이브러리는 인터넷에서 웹 페이지를 가져오는 데 도움을 줍니다. 웹 브라우저와 같지만 파이썬 코드용입니다.\n",
    "\n",
    "### Installing Requests (Requests 설치)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b1d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a8fc4",
   "metadata": {},
   "source": [
    "### Basic Requests Usage (기본 Requests 사용법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get a web page\n",
    "response = requests.get('https://httpbin.org/html')\n",
    "\n",
    "# Check if request was successful\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "\n",
    "# Get the content\n",
    "content = response.text\n",
    "print(\"Page content:\")\n",
    "print(content[:200])  # Show first 200 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f0b4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. HTTP Requests and Responses (HTTP 요청과 응답)\n",
    "\n",
    "When you visit a website, your browser sends a **request** and gets a **response**. Web scraping works the same way.\n",
    "웹사이트를 방문할 때, 브라우저는 **요청**을 보내고 **응답**을 받습니다. 웹 스크래핑도 같은 방식으로 작동합니다.\n",
    "\n",
    "### Understanding Status Codes (상태 코드 이해)\n",
    "\n",
    "- **200**: Success! Page found (성공! 페이지 찾음)\n",
    "- **404**: Page not found (페이지를 찾을 수 없음)\n",
    "- **403**: Access denied (접근 거부)\n",
    "\n",
    "### Simple Status Check (간단한 상태 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_website(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Success! Got {len(response.text)} characters\")\n",
    "        else:\n",
    "            print(f\"Error: Status code {response.status_code}\")\n",
    "    except requests.exceptions.RequestException:\n",
    "        print(\"Could not connect to website\")\n",
    "\n",
    "# Test with a simple website\n",
    "check_website('https://httpbin.org/html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad372db7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Introduction to BeautifulSoup (BeautifulSoup 소개)\n",
    "\n",
    "**BeautifulSoup** helps you find specific information in HTML code. It's like having a smart search tool for web pages.\n",
    "**BeautifulSoup**은 HTML 코드에서 특정 정보를 찾는 데 도움을 줍니다. 웹 페이지를 위한 스마트 검색 도구가 있는 것과 같습니다.\n",
    "\n",
    "### Installing BeautifulSoup (BeautifulSoup 설치)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9862978",
   "metadata": {},
   "source": [
    "### Basic BeautifulSoup Usage (기본 BeautifulSoup 사용법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70391d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get a web page\n",
    "response = requests.get('https://httpbin.org/html')\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the title\n",
    "title = soup.find('title')\n",
    "if title:\n",
    "    print(f\"Page title: {title.text}\")\n",
    "\n",
    "# Find all headings\n",
    "headings = soup.find_all('h1')\n",
    "for heading in headings:\n",
    "    print(f\"Heading: {heading.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d89c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. HTML Parsing and Element Selection (HTML 파싱 및 요소 선택)\n",
    "\n",
    "HTML has tags like `<title>`, `<h1>`, `<p>` that organize content. BeautifulSoup helps you find these tags.\n",
    "HTML에는 내용을 구성하는 `<title>`, `<h1>`, `<p>`와 같은 태그가 있습니다. BeautifulSoup은 이러한 태그를 찾는 데 도움을 줍니다.\n",
    "\n",
    "### Common HTML Tags (일반적인 HTML 태그)\n",
    "\n",
    "- `<title>`: Page title (페이지 제목)\n",
    "- `<h1>, <h2>`: Headings (제목들)\n",
    "- `<p>`: Paragraphs (단락)\n",
    "- `<a>`: Links (링크)\n",
    "- `<img>`: Images (이미지)\n",
    "\n",
    "### Finding Elements (요소 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sample HTML\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head><title>Sample Page</title></head>\n",
    "<body>\n",
    "    <h1>Welcome</h1>\n",
    "    <p>This is a paragraph.</p>\n",
    "    <p>This is another paragraph.</p>\n",
    "    <a href=\"https://example.com\">Click here</a>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find single element\n",
    "title = soup.find('title').text\n",
    "print(f\"Title: {title}\")\n",
    "\n",
    "# Find all paragraphs\n",
    "paragraphs = soup.find_all('p')\n",
    "for i, p in enumerate(paragraphs, 1):\n",
    "    print(f\"Paragraph {i}: {p.text}\")\n",
    "\n",
    "# Find link\n",
    "link = soup.find('a')\n",
    "print(f\"Link text: {link.text}\")\n",
    "print(f\"Link URL: {link.get('href')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f772f27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Web Scraping Ethics and Legal Considerations (웹 스크래핑 윤리와 법적 고려사항)\n",
    "\n",
    "### Important Rules (중요한 규칙)\n",
    "\n",
    "1. **Check robots.txt**: See what's allowed (robots.txt 확인: 허용되는 것 확인)\n",
    "2. **Don't overload servers**: Be polite, add delays (서버 과부하 금지: 예의 지키기, 지연 추가)\n",
    "3. **Respect copyright**: Don't steal content (저작권 존중: 콘텐츠 도용 금지)\n",
    "4. **Read terms of service**: Follow website rules (서비스 약관 읽기: 웹사이트 규칙 따르기)\n",
    "\n",
    "### Best Practices (모범 사례)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def polite_scraping(url):\n",
    "    # Add delay to be polite\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Use headers to identify yourself\n",
    "    headers = {\n",
    "        'User-Agent': 'Educational Bot 1.0'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response\n",
    "    except:\n",
    "        print(\"Could not access website\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa743e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab Exercises (실습)\n",
    "\n",
    "### Lab 1: Simple Web Page Content Extraction (간단한 웹페이지 내용 추출)\n",
    "\n",
    "**Problem**: Extract the title and main heading from a simple web page.\n",
    "문제: 간단한 웹 페이지에서 제목과 주요 헤딩을 추출하세요.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ce4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_basic_info(url):\n",
    "    try:\n",
    "        # Get the web page\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract title\n",
    "            title_tag = soup.find('title')\n",
    "            title = title_tag.text if title_tag else \"No title found\"\n",
    "            \n",
    "            # Extract main heading\n",
    "            h1_tag = soup.find('h1')\n",
    "            heading = h1_tag.text if h1_tag else \"No heading found\"\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"Website: {url}\")\n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Main Heading: {heading}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Error: Could not access {url}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Test with a simple website\n",
    "extract_basic_info('https://httpbin.org/html')\n",
    "\n",
    "# You can test with other simple websites\n",
    "# extract_basic_info('https://example.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc28d4",
   "metadata": {},
   "source": [
    "### Lab 2: News Headline Collector (뉴스 제목 수집기)\n",
    "\n",
    "**Problem**: Create a simple headline collector from a basic HTML structure.\n",
    "문제: 기본 HTML 구조에서 간단한 헤드라인 수집기를 만드세요.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def collect_headlines(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all heading tags (h1, h2, h3)\n",
    "    headlines = []\n",
    "    \n",
    "    for tag in ['h1', 'h2', 'h3']:\n",
    "        elements = soup.find_all(tag)\n",
    "        for element in elements:\n",
    "            headlines.append(element.text.strip())\n",
    "    \n",
    "    return headlines\n",
    "\n",
    "def save_headlines_to_file(headlines, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"Collected Headlines\\n\")\n",
    "        file.write(\"=\" * 20 + \"\\n\\n\")\n",
    "        \n",
    "        for i, headline in enumerate(headlines, 1):\n",
    "            file.write(f\"{i}. {headline}\\n\")\n",
    "    \n",
    "    print(f\"Headlines saved to {filename}\")\n",
    "\n",
    "# Sample HTML with news-like structure\n",
    "sample_html = \"\"\"\n",
    "<html>\n",
    "<head><title>News Site</title></head>\n",
    "<body>\n",
    "    <h1>Breaking News</h1>\n",
    "    <h2>Technology Update</h2>\n",
    "    <h2>Sports Results</h2>\n",
    "    <h3>Weather Forecast</h3>\n",
    "    <h3>Local Events</h3>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Extract headlines\n",
    "headlines = collect_headlines(sample_html)\n",
    "\n",
    "print(\"Found Headlines:\")\n",
    "for i, headline in enumerate(headlines, 1):\n",
    "    print(f\"{i}. {headline}\")\n",
    "\n",
    "# Save to file\n",
    "save_headlines_to_file(headlines, 'headlines.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d4b04",
   "metadata": {},
   "source": [
    "### Lab 3: Product Price Information Scraper (상품 가격 정보 크롤러)\n",
    "\n",
    "**Problem**: Extract product information from a simple HTML structure.\n",
    "문제: 간단한 HTML 구조에서 제품 정보를 추출하세요.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_product_info(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    products = []\n",
    "    \n",
    "    # Look for product containers (divs with class 'product')\n",
    "    product_divs = soup.find_all('div', class_='product')\n",
    "    \n",
    "    for product_div in product_divs:\n",
    "        # Extract product name\n",
    "        name_tag = product_div.find('h3')\n",
    "        name = name_tag.text.strip() if name_tag else \"Unknown\"\n",
    "        \n",
    "        # Extract price\n",
    "        price_tag = product_div.find('span', class_='price')\n",
    "        price = price_tag.text.strip() if price_tag else \"No price\"\n",
    "        \n",
    "        products.append({'name': name, 'price': price})\n",
    "    \n",
    "    return products\n",
    "\n",
    "def save_products_to_csv(products, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"Product Name,Price\\n\")\n",
    "        for product in products:\n",
    "            file.write(f\"{product['name']},{product['price']}\\n\")\n",
    "    \n",
    "    print(f\"Product data saved to {filename}\")\n",
    "\n",
    "# Sample HTML with product structure\n",
    "sample_html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <div class=\"product\">\n",
    "        <h3>Laptop Computer</h3>\n",
    "        <span class=\"price\">$999</span>\n",
    "    </div>\n",
    "    <div class=\"product\">\n",
    "        <h3>Wireless Mouse</h3>\n",
    "        <span class=\"price\">$25</span>\n",
    "    </div>\n",
    "    <div class=\"product\">\n",
    "        <h3>Keyboard</h3>\n",
    "        <span class=\"price\">$75</span>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Extract product information\n",
    "products = extract_product_info(sample_html)\n",
    "\n",
    "print(\"Found Products:\")\n",
    "for product in products:\n",
    "    print(f\"Name: {product['name']}, Price: {product['price']}\")\n",
    "\n",
    "# Save to CSV file\n",
    "save_products_to_csv(products, 'products.csv')\n",
    "\n",
    "print(f\"\\nTotal products found: {len(products)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e4892",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## Quiz Section (퀴즈)\n",
    "\n",
    "### Quiz 1: Basic Web Requests\n",
    "\n",
    "**Question**: Use requests library to get webpage content from 'https://httpbin.org/html' and print the status code. Also print the first 100 characters of the content.\n",
    "\n",
    "requests 라이브러리를 사용하여 'https://httpbin.org/html'에서 웹페이지 내용을 가져오고 상태 코드를 출력하세요. 또한 내용의 첫 100자를 출력하세요.\n",
    "\n",
    "**Write your answer here (답을 여기에 작성하세요)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb874d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cbd152",
   "metadata": {},
   "source": [
    "### Quiz 2: HTML Parsing with BeautifulSoup\n",
    "\n",
    "**Question**: Given the HTML below, use BeautifulSoup to extract and print all title tags. Also extract all paragraph texts.\n",
    "\n",
    "아래 주어진 HTML에서 BeautifulSoup을 사용하여 모든 title 태그를 추출하고 출력하세요. 또한 모든 단락 텍스트도 추출하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a69138",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><title>Test Page</title></head>\n",
    "<body>\n",
    "    <title>Another Title</title>\n",
    "    <p>First paragraph</p>\n",
    "    <p>Second paragraph</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26801c6f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Write your answer here (답을 여기에 작성하세요)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9354bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d467ca",
   "metadata": {},
   "source": [
    "### Quiz 3: News Headlines Scraper\n",
    "\n",
    "**Question**: Write a program that extracts all h1 and h2 headings from HTML content and saves them to a text file called 'news_headlines.txt'. Use the sample HTML provided.\n",
    "\n",
    "HTML 내용에서 모든 h1과 h2 헤딩을 추출하여 'news_headlines.txt'라는 텍스트 파일에 저장하는 프로그램을 작성하세요. 제공된 샘플 HTML을 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07427a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <h1>Main News Story</h1>\n",
    "    <h2>Technology News</h2>\n",
    "    <h2>Sports Update</h2>\n",
    "    <p>Some content here</p>\n",
    "    <h1>Breaking News</h1>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6ee37",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Write your answer here (답을 여기에 작성하세요)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ad722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03451d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References (참고)\n",
    "\n",
    "1. **Requests Documentation**: https://requests.readthedocs.io/en/latest/\n",
    "2. **BeautifulSoup Tutorial**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "3. **Web Scraping Ethics Guide**: https://blog.apify.com/web-scraping-ethics/\n",
    "4. **Real Python Web Scraping**: https://realpython.com/beautiful-soup-web-scraper-python/\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points (핵심 포인트)\n",
    "\n",
    "### Remember (기억하세요)\n",
    "1. **Always check status code**: 200 means success (항상 상태 코드 확인: 200은 성공 의미)\n",
    "2. **Be respectful**: Don't overload websites (예의 지키기: 웹사이트에 과부하 주지 말기)\n",
    "3. **Handle errors**: Use try-except for safety (오류 처리: 안전을 위해 try-except 사용)\n",
    "4. **Follow the law**: Respect robots.txt and terms of service (법 준수: robots.txt와 서비스 약관 존중)\n",
    "\n",
    "### Safety Tips (안전 팁)\n",
    "- Start with simple, educational websites (간단한 교육용 웹사이트부터 시작)\n",
    "- Always add delays between requests (요청 사이에 항상 지연 추가)\n",
    "- Don't scrape personal or sensitive data (개인적이거나 민감한 데이터 스크래핑 금지)\n",
    "- Test your code with small examples first (먼저 작은 예제로 코드 테스트)\n",
    "\n",
    "### Next Week Preview (다음 주 미리보기)\n",
    "Next week: **API Basics and Weather Data** - Getting data through official APIs\n",
    "다음 주: **API 기초 및 날씨 데이터** - 공식 API를 통한 데이터 가져오기\n",
    "\n",
    "---\n",
    "\n",
    "## Homework (숙제)\n",
    "\n",
    "1. Complete all three lab exercises (3개 실습 모두 완료)\n",
    "2. Practice with the sample HTML provided in exercises (실습에서 제공된 샘플 HTML로 연습)\n",
    "3. Research robots.txt for a website you're interested in (관심 있는 웹사이트의 robots.txt 조사)\n",
    "4. Try extracting different HTML elements (div, span, etc.) (다른 HTML 요소들 추출 시도)\n",
    "\n",
    "**Web scraping is powerful, but use it responsibly and ethically!**\n",
    "**웹 스크래핑은 강력하지만 책임감 있고 윤리적으로 사용하세요!**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
